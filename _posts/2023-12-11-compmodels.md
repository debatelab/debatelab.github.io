---
layout: post
title: "Studying Rational Agency and Epistemic Communities with Large Language Models"
author: "Gregor Betz"
categories: talk
tags: [language modeling, AI, epistemology, multi agent models, bounded confidence]
---

[üñ•Ô∏è [Slides]({{ site.baseurl }}/assets/slides/comp-models-23.html)] 
[üìì [Jupyter Notebook](https://github.com/debatelab/genai-epistemology/blob/main/notebooks/bounded_confidence_llm.ipynb)] 

In a keynote at the [Workshop on Computational Models in Social Epistemology 2023](https://www.ruhr-uni-bochum.de/rrs-philosophy/events/comp-models-23.html#ID-86ff5ec8-cbcb-47a9-b5ae-b3a8dc3ac0f1), organised by the RUB [Research Group on Reasoning, Rationality and Science](https://www.ruhr-uni-bochum.de/rrs-philosophy/) as part of the DFG network ‚ÄúSimulations of Scientific Inquiry‚Äù, we've reviewed and critically reflected on the emergent field of LLM-based multi-agent simulations of (individual and collective) epistemic processes.

To get philosophers started in that field, we publish a Jupyter notebook that walks you through an implementation of the classic Bounded Confidence model with LLM-based agents. In that model, agents can process natural language and their opinions are represented by English sentences. The Jupyter notebook may serve as boilerplate for your own projects.
